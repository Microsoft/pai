# Copyright (c) Microsoft Corporation
# All rights reserved.
#
# MIT License
#
# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated
# documentation files (the "Software"), to deal in the Software without restriction, including without limitation
# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and
# to permit persons to whom the Software is furnished to do so, subject to the following conditions:
# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING
# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,
# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

clusterID: phltest 


clusterinfo:

  # HDFS, zookeeper data path
  dataPath: "/datastorage"

  # PAI data root path
  paiPath: "/var/pai"

  # Choose proper nvidia driver version from this url http://www.nvidia.com/object/linux-amd64-display-archive.html
  nvidia_drivers_version: 384.111

  # static docker-version
  # https://download.docker.com/linux/static/stable/x86_64/docker-17.06.2-ce.tgz
  # Docker client used by hadoop NM (node manager) to launch Docker containers (e.g., of a deep learning job) in the host env.
  dockerverison: 17.06.2

  # the docker registry to store docker images that contain system services like frameworklauncher, hadoop, etc.
  dockerregistryinfo:

    # If public, please fill it the same as your username
    docker_namespace: prom11

    # E.g., gcr.io. If publicï¼Œfill docker_registry_domain with word "public"
    # docker_registry_domain: public
    docker_registry_domain: public
    # If the docker registry doesn't require authentication, please leave docker_username and docker_password empty
    docker_username: yanjie55
    docker_password: gaoyanjie

    docker_tag: 0.3.0

    # The name of the secret in kubernetes will be created in your cluster
    # Must be lower case, e.g., regsecret.
    secretname: dockersecret

  # hadoop config information
  hadoopinfo:
    # If custom_hadoop_binary_path is None, script will download a standard version of hadoop binary for you
    # hadoop-version
    # http://archive.apache.org/dist/hadoop/common/hadoop-2.7.2/hadoop-2.7.2.tar.gz
    custom_hadoop_binary_path: /hadoop-binary/hadoop-2.7.2.tar.gz
    hadoopversion: 2.7.2
    configmapname: hadoop-configuration
    hadoop_vip: 10.151.40.133

  # Step 1 of 4 to set up Hadoop queues.
  # Define all virtual clusters, equivalent concept of Hadoop queues.
  # The capacity of each virtual cluster is specified as the percentage of the whole resources in the system.
  # All un-configured resources will go to an auto-generated virtual cluster called 'default'.
  virtualClusters:
    vc1:
      description: VC for Alice's team.
      capacity: 20
    vc2:
      description: VC for Bob's team.
      capacity: 20
    vc3:
      description: VC for Charlie's team.
      capacity: 20

  frameworklauncher:
    frameworklauncher_vip: 10.151.40.133
    frameworklauncher_port: 9086

  restserverinfo:
    # path for rest api server src dir
    src_path: ../rest-server/
    # uri for frameworklauncher webservice
    webservice_uri: http://10.151.40.133:9086
    # uri for hdfs
    hdfs_uri: hdfs://10.151.40.133:9000
    # port for rest api server
    server_port: 9186
    # secret for signing authentication tokens, e.g., "Hello PAI!"
    jwt_secret: "Hello PAI!"
    # authentication database file path relative to PAI data path
    lowdb_path: "rest-server/user.db.json"
    # database admin username
    lowdb_admin: admin
    # database admin password
    lowdb_passwd: asdfgh

  # The config for web portal
  webportalinfo:
    # root url of the rest server
    rest_server_uri: http://10.151.40.133:9186
    # root url of the prometheus server
    prometheus_uri: http://10.151.40.132:9090
    # root url of the grafana portal
    grafana_uri: http://10.151.40.133:3000
    # root url of the k8s dashboard
    k8s_dashboard_uri: http://10.151.40.133:9090
    # root url of the k8s apiserver
    k8s_api_server_uri: http://10.151.40.133:8080
    # port for webportal
    server_port: 9286
    
  # The config for Grafana
  grafanainfo:  
    # The address of the grafana to connect to
    grafana_url: http://10.151.40.133
    # port for grafana
    grafana_port: 3000

  # The config for monitor service
  prometheusinfo:
    # The address of the prometheus to connect to
    prometheus_url: http://10.151.40.132
    # port for prometheus port
    prometheus_port: 9090
    # port for node exporter
    node_exporter_port: 9100
    # cadvisor port
    cadvisor_port: 8089

  pyloninfo:
    # root url of the rest server
    rest_server_uri: http://10.151.40.133:9186
    # root url of the k8s apiserver
    k8s_api_server_uri: http://10.151.40.133:8080
    # root url of the webhdfs api server
    webhdfs_uri: http://10.151.40.133:50070
    # root url of the prometheus server
    prometheus_uri: http://10.151.40.132:9090
    # root url of the k8s dashboard
    k8s_dashboard_uri: http://10.151.40.133:9090
    # root url of grafana
    grafana_uri: http://10.151.40.133:3000
    # root url of the web portal
    webportal_uri: http://10.151.40.133:9286
    # port of pylon
    port: 80

# The detail machine type information in your cluster, each machine type has an entry here
# An Azure VM example
machineinfo:

  NC24R:
    mem: 224
    gpu:
    # type: gpu{type}
      type: teslak80
      count: 4
    cpu:
      vcore: 24
    dataFolder: "/mnt"
    # Note: Up to now, the only supported os version is Ubuntu16.04. Please do not change it here.
    os: ubuntu16.04

  D8SV3:
    mem: 32
    cpu:
      vcore: 8
    dataFolder: "/mnt"
    # Note: Up to now, the only supported os version is Ubuntu16.04. Pls don't change it here.
    os: ubuntu16.04


# The machine list in your cluster
# An example
machinelist:
  # Replace your own hostname here.
  phltestaux1:
    # recommend to be the same as host IP
    nodename: 10.151.40.133
    # Replace your own machine type here
    machinetype: D8SV3
    # Replace your own IP here.
    ip: 10.151.40.133
    # all hadooprole: master will install zk, please specify a unique zkid for each node. Begin at "1".
    zkid: "1"
    # hdfsrole:   master or worker
    # yarnrole:   master or worker
    # zookeeper:  "true"   or not set this value
    # Note: if zookeeper is set, you should set zkid in this machine.
    # jobhistory: "true"   or not set this value
    # grafana: "true"   or not set this value
    # pylon: "true"   or not set this value
    hdfsrole: master
    yarnrole: master
    zookeeper: "true"
    jobhistory: "true"
    launcher: "true"
    restserver: "true"
    webportal: "true"
    grafana: "true"
    pylon: "true"
  
  # Do the same work as the machine above.
  phltest1:
    nodename: 10.151.40.131
    machinetype: NC24R
    ip: 10.151.40.131
    hdfsrole: worker
    yarnrole: worker
    node-exporter: "true"
    #prometheus: "true"
  # Do the same work as the machine above.
  phltest2:
    nodename: 10.151.40.132
    machinetype: NC24R
    ip: 10.151.40.132
    hdfsrole: worker
    yarnrole: worker
    node-exporter: "true"
    prometheus: "true"
