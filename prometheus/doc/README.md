# Goal

Monitoring all compoments in pai, provide insight on detectiving system/hardware failuring and
analysing jobs performance.

To accomplish the goal, this module should be:
* Robust: monitoring should try its best to not crash. Other compoments/people relay on monitoring system to find root causes or trigger alert.
* Minimize performance impact.
* Minimize latency between metrics generation and metrics consumption.

# Architecture

![Architecture](architecture.png)

We have three parts consisting Pai's monitoring system: `watchdog`, `exporter` and
[`prometheus`](https://prometheus.io/).

`prometheus` is an open source solution for metrics collecting, storage and querying.

`watchdog` is a single instance pod responsible for monitoring k8s/nodes/pods health, it will first
list all pod/node from kubernetes api server and check their status and log their status, this is very
helpful for debugging.

`exporter` are those pods running in the lefe side of node, they are responsible for collecting
metrics from jobs/nodes/gpus. There are two containers running inside `exporter` pod: `gpu_exporter`
and [`node_exporter`](https://github.com/prometheus/node_exporter): `gpu_exporter` exposes job/gpu
metrics to volume mounted in `/datastorage/prometheus`.

Metrics generated by `watchdog` and `gpu_exporter` are collected by `node_exporter` container running
inside `exporter` pod. Those metrics are scraped by `node_exporter` container. `node_exporter` also
expose node metricss like node cpu/memory/disk usage.

# Metrics collected

Some important metrics are listed below.

| Metrics Name | By | Description |
| --- | --- | --- |
| `nvidiasmi_attached_gpus` | `gpu_exporter` | number of gpu detectived by nvidiasmi |
| `nvidiasmi_utilization_gpu` | `gpu_exporter` | GPU utilization detectived by nvidiasmi |
| `nvidiasmi_utilization_memory` | `gpu_exporter` | GPU memory utilization detectiving by nvidiasmi |
| `container_GPUPerc` | `gpu_exporter` | GPU utilization by specified container |
| `container_GPUMemPerc` | `gpu_exporter` | GPU memory utilization by specified container |
| `container_CPUPerc` | `gpu_exporter` | CPU utilization detectived by docker stats |
| `container_MemUsage` | `gpu_exporter` | Memory usage detectived by docker stats (byte) |
| `container_MemLimit` | `gpu_exporter` | Memory limit detectived by docker stats (byte) |
| `container_MemPerc` | `gpu_exporter` | Memory utilization detectived by docker stats |
| `container_NetIn` | `gpu_exporter` | Network in traffic detectived by docker stats (byte) |
| `container_NetOut` | `gpu_exporter` | Network out traffic detectived by docker stats (byte) |
| `container_BlockIn` | `gpu_exporter` | Block io in traffic detectived by docker stats (byte) |
| `container_BlockOut` | `gpu_exporter` | Block io out traffic detectived by docker stats (byte) |
| `node_filefd_allocated` | `node_exporter` | Number of file descriptor allocated in node |
| `node_disk_read_time_ms` | `node_exporter` | disk read time (ms) |
| `node_disk_write_time_ms` | `node_exporter` | disk write time (ms) |
| `node_load1` | `node_exporter` | node load in past 1 minute |
| `node_filesystem_free` | `node_exporter` | filesystem free space (byte) |

More metrics are listed [here](./watchdog-metrics.md).

# Build

Build image by using `paictl.py`:
```
./paictl.py image build -p ~/pai-config/ -n gpu-exporter
```

push to registry for deploying:

```
./paictl.py image push -p ~/pai-config/ -n gpu-exporter
```

# Deployment

start using `paictl.py`:

```
./paictl.py service start -p ~/pai-config/ -n prometheus
```

# TODO
* modulize `gpu_exporter` and `watchdog`, make it easier for adding unit test
* expose metrics collected by watchdog through server instead of file
* use tmpfs as intermediate filesystem for `gpu_exporter` to export metrics
